{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "## Graph Neural Networks with Pytorch\r\n",
                "## Target: GraphSAGE\r\n",
                "- Original Paper: https://arxiv.org/abs/1706.02216\r\n",
                "- Original Code: https://github.com/rusty1s/pytorch_geometric/blob/master/examples/reddit.py"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import os\r\n",
                "import sys\r\n",
                "\r\n",
                "import torch\r\n",
                "import torch.nn.functional as F\r\n",
                "from tqdm import tqdm\r\n",
                "from torch_geometric.datasets import Reddit\r\n",
                "from torch_geometric.data import NeighborSampler\r\n",
                "from torch_geometric.nn import SAGEConv\r\n",
                "\r\n",
                "sys.path.append('../')\r\n",
                "from utils import *\r\n",
                "logger = make_logger(name='graphsage_logger')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "# Load Reddit Dataset\r\n",
                "path = os.path.join(os.getcwd(), '..', 'data', 'Reddit')\r\n",
                "dataset = Reddit(path)\r\n",
                "data = dataset[0]"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "# Data 확인\r\n",
                "# Nodes: 232965, Node Features: 602\r\n",
                "logger.info(f\"Node Feature Matrix Info: # Nodes: {data.x.shape[0]}, # Node Features: {data.x.shape[1]}\")\r\n",
                "\r\n",
                "# Edge Index\r\n",
                "# Graph Connectivity in COO format with shape (2, num_edges) = (2, 114,615,892=1.14억)\r\n",
                "logger.info(f\"Edge Index Shape: {data.edge_index.shape}\")\r\n",
                "logger.info(f\"Edge Weight: {data.edge_attr}\")\r\n",
                "\r\n",
                "# train_mask denotes against which nodes to train (153431 nodes)\r\n",
                "print(data.train_mask.sum().item())"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "2021-08-14 20:45:58,322 - graphsage_logger - Node Feature Matrix Info: # Nodes: 232965, # Node Features: 602\n",
                        "2021-08-14 20:45:58,323 - graphsage_logger - Edge Index Shape: torch.Size([2, 114615892])\n",
                        "2021-08-14 20:45:58,323 - graphsage_logger - Edge Weight: None\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "153431\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "# Define Sampler\r\n",
                "train_loader = NeighborSampler(\r\n",
                "    data.edge_index, node_idx=data.train_mask,\r\n",
                "    sizes=[25, 10], batch_size=1024, shuffle=True, num_workers=12)\r\n",
                "\r\n",
                "subgraph_loader = NeighborSampler(\r\n",
                "    data.edge_index, node_idx=None,\r\n",
                "    sizes=[-1], batch_size=1024, shuffle=False, num_workers=12)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "# Look\r\n",
                "batch_size, n_id, adjs = next(iter(train_loader))\r\n",
                "\r\n",
                "# 1) batch_size\r\n",
                "# 현재 batch size를 의미함 (integer)\r\n",
                "logger.info(f\"Current Batch Size: {batch_size}\")\r\n",
                "\r\n",
                "# 2) n_id\r\n",
                "# 이번 Subgraph에서 사용된 모든 node id\r\n",
                "# batch_size개의 Row를 예측하기 위해서 이에 대한 1차 이웃 node A개가 필요하고\r\n",
                "# 1차 이웃 node A개를 위해서는 2차 이웃 node B개가 필요함\r\n",
                "# n_id.shape = batch_size + A + B\r\n",
                "logger.info(f\"현재 Subgraph에서 사용된 모든 node id의 개수: {n_id.shape[0]}\")\r\n",
                "\r\n",
                "# 3) adjs\r\n",
                "# 아래와 같이 Layer의 수가 2개이면 adjs는 길이 2의 List가 된다.\r\n",
                "# head node가 있고 1-hop neighbors와 2-hop neighbors가 있다고 할 때\r\n",
                "# adjs[1]이 head node와 1-hop neighbors의 관계를 설명하며  (1번째 Layer)\r\n",
                "# adjs[0]이 1-hop neighbors와 2-hop neighbors의 관계를 설명한다. (2번째 Layer)\r\n",
                "logger.info(f\"Layer의 수: {len(adjs)}\")\r\n",
                "\r\n",
                "# 각 리스트에는 아래와 같은 튜플이 들어있다.\r\n",
                "# (edge_index, e_id, size)\r\n",
                "# edge_index: source -> target nodes를 기록한 bipartite edges\r\n",
                "# e_id: 위 edge_index에 들어있는 index가 Full Graph에서 갖는 node id\r\n",
                "\r\n",
                "# size: 위 edge_index에 들어있는 node의 수를 튜플로 나타낸 것으로\r\n",
                "# head -> 1-hop 관계를 예시로 들면,\r\n",
                "# head node의 수가 a개, 1-hop node의 수가 b개라고 했을 때\r\n",
                "# size = (a+b, a)\r\n",
                "# 또한 target node의 경우 source nodes의 리스트의 시작 부분에 포함되어 있어\r\n",
                "# skip-connections나 self-loops를 쉽게 사용할 수 있게 되어 있음\r\n",
                "A = adjs[1].size[0] - batch_size\r\n",
                "B = adjs[0].size[0] - A - batch_size\r\n",
                "\r\n",
                "logger.info(f\"진행 방향: {B}개의 2-hop neighbors ->\"\r\n",
                "            f\"{A}개의 1-hop neighbors -> {batch_size}개의 Head Nodes\")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "2021-08-14 20:49:39,381 - graphsage_logger - Current Batch Size: 1024\n",
                        "2021-08-14 20:49:39,382 - graphsage_logger - 현재 Subgraph에서 사용된 모든 node id의 개수: 107326\n",
                        "2021-08-14 20:49:39,383 - graphsage_logger - Layer의 수: 2\n",
                        "2021-08-14 20:49:39,383 - graphsage_logger - 진행 방향: 85364개의 2-hop neighbors ->20938개의 1-hop neighbors -> 1024개의 Head Nodes\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "# Define Model\r\n",
                "class SAGE(torch.nn.Module):\r\n",
                "    def __init__(self, in_channels, hidden_channels, out_channels):\r\n",
                "        super(SAGE, self).__init__()\r\n",
                "\r\n",
                "        self.num_layers = 2\r\n",
                "\r\n",
                "        self.convs = torch.nn.ModuleList()\r\n",
                "        self.convs.append(SAGEConv(in_channels, hidden_channels))\r\n",
                "        self.convs.append(SAGEConv(hidden_channels, out_channels))\r\n",
                "\r\n",
                "    def forward(self, x, adjs):\r\n",
                "        for i, (edge_index, _, size) in enumerate(adjs):\r\n",
                "            x_target = x[:size[1]]  # Target nodes are always placed first.\r\n",
                "            x = self.convs[i]((x, x_target), edge_index)\r\n",
                "\r\n",
                "            # 마지막 Layer는 Dropout을 적용하지 않는다.\r\n",
                "            if i != self.num_layers - 1:\r\n",
                "                x = F.relu(x)\r\n",
                "                x = F.dropout(x, p=0.5, training=self.training)\r\n",
                "        return x.log_softmax(dim=-1)\r\n",
                "\r\n",
                "    def inference(self, x_all):\r\n",
                "        pbar = tqdm(total=x_all.size(0) * self.num_layers)\r\n",
                "        pbar.set_description('Evaluating')\r\n",
                "\r\n",
                "        # Compute representations of nodes layer by layer, using *all*\r\n",
                "        # available edges. This leads to faster computation in contrast to\r\n",
                "        # immediately computing the final representations of each batch.\r\n",
                "        for i in range(self.num_layers):\r\n",
                "            xs = []\r\n",
                "            for batch_size, n_id, adj in subgraph_loader:\r\n",
                "                edge_index, _, size = adj.to(device)\r\n",
                "                x = x_all[n_id].to(device)\r\n",
                "                x_target = x[:size[1]]\r\n",
                "                x = self.convs[i]((x, x_target), edge_index)\r\n",
                "                if i != self.num_layers - 1:\r\n",
                "                    x = F.relu(x)\r\n",
                "                xs.append(x.cpu())\r\n",
                "\r\n",
                "                pbar.update(batch_size)\r\n",
                "\r\n",
                "            x_all = torch.cat(xs, dim=0)\r\n",
                "\r\n",
                "        pbar.close()\r\n",
                "        return x_all"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "**GraphSAGE**는 이제는 굉장히 Classic하고 기본적인 방법론으로 분류되며 이 알고리즘의 단점을 보완한 후속 연구도 다수 진행되었음에도 불구하고,  \r\n",
                "Large Scale을 가진 데이터에 효과적인 접근 방법으로 활용할 수 있다는 점에서 여전히 매우 가치 있다고 볼 수 있다.  \r\n",
                "\r\n",
                "**SAGEConv** Layer는 다음과 같은 식을 구현한 것이다.  \r\n",
                "\r\n",
                "$$ h_v^k \\leftarrow \\sigma ( \\mathbf{W} \\cdot AGG( \\{ h_v^{k-1} \\} \\cup \\{ h_u^{k-1} \\} ), $$  \r\n",
                "\r\n",
                "$$ \\forall u \\in \\mathcal{N}(v) $$  \r\n",
                "\r\n",
                "참고로 논문 원본에서는 위 `AGG`에 해당하는 Aggregator로 Mean Aggregator, LSTM Aggregator, Pooling Aggregator 등을 제시하였는데,  \r\n",
                "\r\n",
                "`torch_geometric.nn`에서는 Mean Aggregator만 구현되어 있다. 본 논문에 대해서 자세히 알고 싶다면 [논문 원본](https://arxiv.org/abs/1706.02216)을 참고하여도 좋고, [논문 리뷰 글](https://greeksharifa.github.io/machine_learning/2020/12/31/Graph-Sage/)을 참고하여도 좋다.  \r\n",
                "\r\n",
                "**Mean Aggregator**는 효과적인 경우도 많지만 Simplicity를 향상시킨 것과 반대로 모델의 표현력은 크게 감소시키는 한계를 갖는다.  \r\n",
                "\r\n",
                "이에 대한 고찰과 해결 방법을 제시한 논문이 [Graph Isomorphism Networks]이며 이에 대한 예시 노트북은 [이 곳](https://github.com/ocasoyy/pytorch_graph_neural_networks/blob/main/gin/gin.ipynb)에서 확인할 수 있고, [논문 리뷰 글](https://greeksharifa.github.io/machine_learning/2021/06/05/GIN/) 또한 확인하 수 있으니 참고하면 좋을 것이다.  \r\n",
                "\r\n",
                "**SAGEConv** Layer에 대한 간략한 설명은 아래와 같다.  \r\n",
                "\r\n",
                "`in_channels`와 `out_channels`는 당연히 Input 및 Output의 Size를 의미한다.  \r\n",
                "\r\n",
                "`normalize`인자의 기본 값은 `False`이며 만약 `True`로 설정할 경우 Out Feature에는 L2 Normalize가 적용된다.  \r\n",
                "\r\n",
                "최종 Output을 이용하여 코사인 유사도를 구하고자 한다면 최종 Layer에 대해서 `normalize=True`로 설정하면 효율적인 코딩이 가능할 것을 보인다.  \r\n",
                "\r\n",
                "`root_weight` 인자의 기본 값은 `True`이며 만약 `False`로 설정할 경우 Feature를 Aggregate 하는 과정에서 Root Node(Head Node)의 Feature는 포함되지 않을 것이다.  \r\n",
                "\r\n",
                "이해 대한 세부적인 논리를 이해하기 위해서는 논문 원본을 참고하는 것을 추천한다.  \r\n",
                "\r\n",
                "`bias`의 기본 값은 `True`이며 `False`로 설정할 경우 Linear Layer에서 Bias는 제거될 것이다.  \r\n",
                "\r\n",
                "`forward` 메서드를 사용하기 위해서는 `x`와 `edge_index`를 입력해 주어야 한다.  \r\n",
                "\r\n",
                "공식 문서 설명은 [이 곳](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.SAGEConv)에서 확인할 수 있다.  "
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
                "model = SAGE(dataset.num_features, 256, dataset.num_classes)\r\n",
                "model = model.to(device)\r\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\r\n",
                "\r\n",
                "x = data.x.to(device)\r\n",
                "y = data.y.squeeze().to(device)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "def train(epoch):\r\n",
                "    model.train()\r\n",
                "\r\n",
                "    pbar = tqdm(total=int(data.train_mask.sum()))\r\n",
                "    pbar.set_description(f'Epoch {epoch:02d}')\r\n",
                "\r\n",
                "    total_loss = total_correct = 0\r\n",
                "    for batch_size, n_id, adjs in train_loader:\r\n",
                "        # `adjs` holds a list of `(edge_index, e_id, size)` tuples.\r\n",
                "        adjs = [adj.to(device) for adj in adjs]\r\n",
                "\r\n",
                "        optimizer.zero_grad()\r\n",
                "        out = model(x[n_id], adjs)\r\n",
                "        loss = F.nll_loss(out, y[n_id[:batch_size]])\r\n",
                "        loss.backward()\r\n",
                "        optimizer.step()\r\n",
                "\r\n",
                "        total_loss += float(loss)\r\n",
                "        total_correct += int(out.argmax(dim=-1).eq(y[n_id[:batch_size]]).sum())\r\n",
                "        pbar.update(batch_size)\r\n",
                "\r\n",
                "    pbar.close()\r\n",
                "\r\n",
                "    loss = total_loss / len(train_loader)\r\n",
                "    approx_acc = total_correct / int(data.train_mask.sum())\r\n",
                "    return loss, approx_acc\r\n",
                "\r\n",
                "\r\n",
                "@torch.no_grad()\r\n",
                "def test():\r\n",
                "    model.eval()\r\n",
                "    out = model.inference(x)\r\n",
                "\r\n",
                "    y_true = y.cpu().unsqueeze(-1)\r\n",
                "    y_pred = out.argmax(dim=-1, keepdim=True)\r\n",
                "\r\n",
                "    results = []\r\n",
                "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\r\n",
                "        results += [int(y_pred[mask].eq(y_true[mask]).sum()) / int(mask.sum())]\r\n",
                "    return results"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "for epoch in range(1, 3):\r\n",
                "    loss, acc = train(epoch)\r\n",
                "    print(f'Epoch {epoch:02d}, Loss: {loss:.4f}, Approx. Train: {acc:.4f}')\r\n",
                "    # train_acc, val_acc, test_acc = test()\r\n",
                "    # print(f'Train: {train_acc:.4f}, Val: {val_acc:.4f}, Test: {test_acc:.4f}')"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "Epoch 01: 100%|██████████| 153431/153431 [00:20<00:00, 7352.91it/s] \n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Epoch 01, Loss: 0.5209, Approx. Train: 0.8919\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "Epoch 02: 100%|██████████| 153431/153431 [00:19<00:00, 7685.09it/s] "
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Epoch 02, Loss: 0.4125, Approx. Train: 0.9260\n"
                    ]
                },
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "\n"
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.6 64-bit ('venv': venv)"
        },
        "interpreter": {
            "hash": "a0c8d12882c844609327ff267203d9c0214a9c81f212141ad6ff46d6cf6ed682"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}