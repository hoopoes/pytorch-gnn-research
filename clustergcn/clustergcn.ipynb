{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "## Graph Neural Networks with Pytorch\r\n",
                "## Target: ClusterGCN\r\n",
                "- Original Paper: https://arxiv.org/abs/1905.07953\r\n",
                "- Original Code: https://github.com/rusty1s/pytorch_geometric/blob/master/examples/cluster_gcn_ppi.py"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "import os\r\n",
                "import sys\r\n",
                "\r\n",
                "import torch\r\n",
                "import torch.nn.functional as F\r\n",
                "from torch_geometric.datasets import PPI\r\n",
                "from torch_geometric.nn import SAGEConv, BatchNorm\r\n",
                "from torch_geometric.data import Batch, ClusterData, ClusterLoader, DataLoader\r\n",
                "from sklearn.metrics import f1_score\r\n",
                "\r\n",
                "sys.path.append('../')\r\n",
                "from utils import *\r\n",
                "logger = make_logger(name='clustergcn_logger')\r\n",
                "\r\n",
                "# Load Dataset\r\n",
                "path = os.path.join(os.getcwd(), 'data', 'PPI')\r\n",
                "train_dataset = PPI(path, split='train')\r\n",
                "val_dataset = PPI(path, split='val')\r\n",
                "test_dataset = PPI(path, split='test')\r\n",
                "\r\n",
                "logger.info(f\"Save Directory: {train_dataset.processed_dir}\")"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "train_data = Batch.from_data_list(train_dataset)\r\n",
                "\r\n",
                "# adj of cluster_data: cluster_data.data.adj\r\n",
                "# you can check partition information in cluster_data.partptr\r\n",
                "cluster_data = ClusterData(\r\n",
                "    train_data, num_parts=50, recursive=False, save_dir=train_dataset.processed_dir)\r\n",
                "train_loader = ClusterLoader(\r\n",
                "    cluster_data, batch_size=1, shuffle=True, num_workers=12)\r\n",
                "\r\n",
                "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\r\n",
                "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\r\n",
                "\r\n",
                "print(cluster_data)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "\r\n",
                "# Define Model\r\n",
                "class ClusterGCN(torch.nn.Module):\r\n",
                "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers):\r\n",
                "        super(ClusterGCN, self).__init__()\r\n",
                "        self.convs = torch.nn.ModuleList()\r\n",
                "        self.batch_norms = torch.nn.ModuleList()\r\n",
                "        self.convs.append(SAGEConv(in_channels, hidden_channels))\r\n",
                "        self.batch_norms.append(BatchNorm(hidden_channels))\r\n",
                "        for _ in range(num_layers - 2):\r\n",
                "            self.convs.append(SAGEConv(hidden_channels, hidden_channels))\r\n",
                "            self.batch_norms.append(BatchNorm(hidden_channels))\r\n",
                "        self.convs.append(SAGEConv(hidden_channels, out_channels))\r\n",
                "\r\n",
                "    def forward(self, x, edge_index):\r\n",
                "        for conv, batch_norm in zip(self.convs[:-1], self.batch_norms):\r\n",
                "            x = conv(x, edge_index)\r\n",
                "            x = batch_norm(x)\r\n",
                "            x = F.relu(x)\r\n",
                "            x = F.dropout(x, p=0.2, training=self.training)\r\n",
                "        return self.convs[-1](x, edge_index)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "**Cluster-GCN**은 **Classic GCN**, **GraphSAGE**, **FastGCN**, **VR-GCN** 등에서 나타난 속도, 메모리 문제 등을 군집화 알고리즘을 통해 논리적으로 풀어낸 방법론이라고 할 수 있다.  \r\n",
                "\r\n",
                "논문 원본은 [이 곳](https://arxiv.org/abs/1905.07953)을 참조하면 좋고, 한국어로 작성한 리뷰 글은 [이 곳](https://greeksharifa.github.io/machine_learning/2021/08/15/ClusterGCN/)에서 확인할 수 있다.  \r\n",
                "\r\n",
                "전체 그래프를 `METIS`라는 군집화 알고리즘을 통해 아래와 같이 복수의 파티션으로 나눈 후,  \r\n",
                "\r\n",
                "$$ \\bar{G} = [\\mathcal{G}_1, ..., \\mathcal{G}_c] = [\\{ \\mathcal{V}_1, \\mathcal{E}_1\\}, ...] $$  \r\n",
                "\r\n",
                "각 파티션을 mini-batch로 취급하여 학습을 수행하는 것이 **Cluster-GCN**의 핵심 아이디어이다. 뛰어난 성능을 유지하면서도 속도, 메모리 측면에서도 괄목할만한 성과를 보여준 방법론이라고 할 수 있다.  "
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "`ClusterData`는 군집화를 통해 graph data object를 복수의 subgraph로 나눠주는 역할을 수행한다. `data`에 `torch_geometric.data.Data` 인스턴스를 입력하면 되고 `num_parts` 인자에 원하는 파티션의 수를 입력하면 된다. `save_dir`에서는 나뉜 데이터를 새로 저장할 주소를 입력하면 된다. 기본값은 None이다.  \r\n",
                "\r\n",
                "`ClusterData`의 결과물을 받았다면, 이를 `ClusterLoader`에 넣어주면 위 코드의 `train_loader`에 해당하는 결과물을 얻을 수 있다."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
                "model = ClusterGCN(in_channels=train_dataset.num_features, hidden_channels=1024,\r\n",
                "            out_channels=train_dataset.num_classes, num_layers=6).to(device)\r\n",
                "loss_op = torch.nn.BCEWithLogitsLoss()\r\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def train():\r\n",
                "    model.train()\r\n",
                "\r\n",
                "    total_loss = 0\r\n",
                "    for data in train_loader:\r\n",
                "        data = data.to(device)\r\n",
                "        optimizer.zero_grad()\r\n",
                "        loss = loss_op(model(data.x, data.edge_index), data.y)\r\n",
                "        loss.backward()\r\n",
                "        optimizer.step()\r\n",
                "        total_loss += loss.item() * data.num_nodes\r\n",
                "    return total_loss / train_data.num_nodes\r\n",
                "\r\n",
                "\r\n",
                "@torch.no_grad()\r\n",
                "def test(loader):\r\n",
                "    model.eval()\r\n",
                "\r\n",
                "    ys, preds = [], []\r\n",
                "    for data in loader:\r\n",
                "        ys.append(data.y)\r\n",
                "        out = model(data.x.to(device), data.edge_index.to(device))\r\n",
                "        preds.append((out > 0).float().cpu())\r\n",
                "\r\n",
                "    y, pred = torch.cat(ys, dim=0).numpy(), torch.cat(preds, dim=0).numpy()\r\n",
                "    return f1_score(y, pred, average='micro') if pred.sum() > 0 else 0\r\n",
                "\r\n",
                "\r\n",
                "for epoch in range(1, 201):\r\n",
                "    loss = train()\r\n",
                "    val_f1 = test(val_loader)\r\n",
                "    test_f1 = test(test_loader)\r\n",
                "    print('Epoch: {:02d}, Loss: {:.4f}, Val: {:.4f}, Test: {:.4f}'.format(\r\n",
                "        epoch, loss, val_f1, test_f1))"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}