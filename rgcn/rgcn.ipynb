{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "## Graph Neural Networks with Pytorch\r\n",
                "## Target: RGCN: Modeling Relational Data with Graph Convolutional Networks\r\n",
                "- Original Paper: https://arxiv.org/abs/1703.06103\r\n",
                "- Original Code: [Example1](https://github.com/rusty1s/pytorch_geometric/blob/master/examples/rgcn.py), [Example2](https://github.com/rusty1s/pytorch_geometric/blob/master/examples/rgcn_link_pred.py)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import os, sys\r\n",
                "\r\n",
                "from tqdm import tqdm\r\n",
                "\r\n",
                "import torch\r\n",
                "import torch.nn.functional as F\r\n",
                "from torch.nn import Parameter\r\n",
                "\r\n",
                "from torch_geometric.nn import GAE, RGCNConv\r\n",
                "\r\n",
                "try:\r\n",
                "    from torch_geometric.datasets import RelLinkPredDataset\r\n",
                "except ImportError:\r\n",
                "    from fb_dataset import RelLinkPredDataset"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "본 노트북에서는 **RGCN**이란 알고리즘에 대해 알아보고, 간단히 코드를 실행시켜 볼 것이다.  \r\n",
                "\r\n",
                "Knowledge bases에서 missing information에 대해 예측하는 것은 SRL(Statistical Relational Learning)에서 굉장히 중요한 문제이다. 이 때 knowledge bases는 (subject, predicate, object)과 같이 collections of triples의 정보를 저장한다.  \r\n",
                "\r\n",
                "예를 들어, (youyoung, was_in_the, room)에서 subject인 youyoung과 object인 room은 entity이고, predicate인 was_in_the는 relation이 된다. 그리고 이 entity는 특정 type을 갖게 된다.  \r\n",
                "\r\n",
                "위와 같은 형태의 관계를 그래프로 나타낸 것이 **Relational Graph**이고, node는 entity, edge는 relation이 된다.  "
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "**RGCN**의 layer update 식은 아래와 같다.  \r\n",
                "\r\n",
                "$$ h_i^{l+1} = \\sigma( \\Sigma_{r \\in \\mathcal{R}} \\Sigma_{j \\in \\mathcal{N}_i^r} \\frac{1}{c_{i, r}} W_r^l h_j^l + W_o^l h_i^l) $$  \r\n",
                "\r\n",
                "이 때 $c_{i, r}$ 의 경우 $\\vert \\mathcal{N}_i^r \\vert$ 와 같이 상수로 정할 수도 있고, 학습 가능한 attention score로 설정할 수도 있다. 위 식은 실제 구현될 때는 행렬식으로 변경해 주어야 한다.  \r\n",
                "\r\n",
                "일단 학습 데이터를 불러와보자."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "path = os.path.join(os.getcwd(), '..', 'data', 'RLPD')\r\n",
                "dataset = RelLinkPredDataset(path, 'FB15k-237')\r\n",
                "data = dataset[0]\r\n",
                "\r\n",
                "print(data)\r\n",
                "print(f\"\\n Num Relations: {dataset.num_relations}\")"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Data(edge_index=[2, 544230], edge_type=[544230], test_edge_index=[2, 20466], test_edge_type=[20466], train_edge_index=[2, 272115], train_edge_type=[272115], valid_edge_index=[2, 17535], valid_edge_type=[17535])\n",
                        "\n",
                        " Num Relations: 474\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "**RGCN**는 목표 task에 따라 구현 방식이 달라진다. Entity Classification을 기준으로 코드가 짜여져 있는 것이고, Link Prediction을 목표로 할 경우 RGCN Encoder + DistMult Decoder의 형식을 갖추게 된다. 본 노트북에서는 후자에 대해 살펴본다.  \r\n",
                "\r\n",
                "모델의 특성 상 relation의 종류가 굉장히 많은 데이터에 적용할 경우 파라미터의 수가 크게 증가하고 이러한 특징은 과적합으로 이어질 가능성이 있다. 이에 대응하기 위해 **RGCN**은 2가지 규제 방안을 제시하고 있다. 적용해본 후에 더 나은 선택지를 고르면 될 것이다. (굉장히 흥미로운 방법이다.)  \r\n",
                "\r\n",
                "1번째 방법은 `Basis Decomposition`이다.  \r\n",
                "\r\n",
                "$$ W_r^l = \\Sigma_{b=1}^B a_{rb}^l V_b^l $$  \r\n",
                "\r\n",
                "$V_b^l$ 은 relation에 의존하지 않고 오직 $a$ 만 relation에 따라 늘어나게 된다. 즉 이 방법은 **Weight Sharing**을 의미한다. 전체적으로 $V$ 를 학습하는데에 초점을 두고, relation에 따라 $a$ 로 조절하게 되는 것이다.  \r\n",
                "\r\n",
                "2번째 방법은 **Block-diagonal Matrices**를 이용하는 것이다.  \r\n",
                "\r\n",
                "$$ W_r^l = \\bigoplus_{b=1}^B Q_{br}^l $$  \r\n",
                "\r\n",
                "이 때 $Q$ 는 block-diagonal 행렬을 의미하게 된다. $W$ 의 shape이 $(d^{l+1}, d^l)$ 이라고 할 때, $Q$ 의 shape은 (d^{l+1}/B, d^l/B)가 된다.  \r\n",
                "\r\n",
                "$$\r\n",
                "\\begin{bmatrix}\r\n",
                "Q_{1r}^l & & \\\\\r\n",
                "& \\ddots & \\\\\r\n",
                "& & Q_{Br}^l\r\n",
                "\\end{bmatrix}\r\n",
                "$$\r\n",
                "\r\n",
                "이렇게 되면 Sparsity Constraint의 효과를 갖게 되면서 Weigth Matrix를 regularize하게 된다. 다만 근접한 차원만이 $W$ 와 interact할 수 있다는 한계를 지니게 된다.  "
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "`RGCONConv`에 대해서는 [공식 문서](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.RGCNConv)에서 자세한 설명을 확인할 수 있다.  \r\n",
                "\r\n",
                "아래 코드를 보면 argument는 매우 직관적임을 알 수 있다."
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "conv = RGCNConv(\r\n",
                "    in_channels=data.num_nodes, out_channels=16, num_relations=dataset.num_relations,\r\n",
                "    num_bases=30, num_blocks=None, aggr='mean')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "class RGCNEncoder(torch.nn.Module):\r\n",
                "    def __init__(self, num_nodes, hidden_channels, num_relations):\r\n",
                "        super().__init__()\r\n",
                "        self.node_emb = Parameter(torch.Tensor(num_nodes, hidden_channels))\r\n",
                "        self.conv1 = RGCNConv(hidden_channels, hidden_channels, num_relations, num_blocks=5)\r\n",
                "        self.conv2 = RGCNConv(hidden_channels, hidden_channels, num_relations, num_blocks=5)\r\n",
                "        self.reset_parameters()\r\n",
                "\r\n",
                "    def reset_parameters(self):\r\n",
                "        torch.nn.init.xavier_uniform_(self.node_emb)\r\n",
                "        self.conv1.reset_parameters()\r\n",
                "        self.conv2.reset_parameters()\r\n",
                "\r\n",
                "    def forward(self, edge_index, edge_type):\r\n",
                "        x = self.node_emb\r\n",
                "        x = self.conv1(x, edge_index, edge_type).relu_()\r\n",
                "        x = F.dropout(x, p=0.2, training=self.training)\r\n",
                "        x = self.conv2(x, edge_index, edge_type)\r\n",
                "        return x"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "class DistMultDecoder(torch.nn.Module):\r\n",
                "    def __init__(self, num_relations, hidden_channels):\r\n",
                "        super().__init__()\r\n",
                "        self.rel_emb = Parameter(torch.Tensor(num_relations, hidden_channels))\r\n",
                "        self.reset_parameters()\r\n",
                "\r\n",
                "    def reset_parameters(self):\r\n",
                "        torch.nn.init.xavier_uniform_(self.rel_emb)\r\n",
                "\r\n",
                "    def forward(self, z, edge_index, edge_type):\r\n",
                "        z_src, z_dst = z[edge_index[0]], z[edge_index[1]]\r\n",
                "        rel = self.rel_emb[edge_type]\r\n",
                "        return torch.sum(z_src * rel * z_dst, dim=1)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "model = GAE(\r\n",
                "    RGCNEncoder(data.num_nodes, hidden_channels=500, num_relations=dataset.num_relations),\r\n",
                "    DistMultDecoder(dataset.num_relations // 2, hidden_channels=500),\r\n",
                ")\r\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "print(model)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "GAE(\n",
                        "  (encoder): RGCNEncoder(\n",
                        "    (conv1): RGCNConv(500, 500, num_relations=474)\n",
                        "    (conv2): RGCNConv(500, 500, num_relations=474)\n",
                        "  )\n",
                        "  (decoder): DistMultDecoder()\n",
                        ")\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "def negative_sampling(edge_index, num_nodes):\r\n",
                "    # Sample edges by corrupting either the subject or the object of each edge.\r\n",
                "    mask_1 = torch.rand(edge_index.size(1)) < 0.5\r\n",
                "    mask_2 = mask_1\r\n",
                "\r\n",
                "    neg_edge_index = edge_index.clone()\r\n",
                "    neg_edge_index[0, mask_1] = torch.randint(num_nodes, (mask_1.sum(), ))\r\n",
                "    neg_edge_index[1, mask_2] = torch.randint(num_nodes, (mask_2.sum(), ))\r\n",
                "    return neg_edge_index"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "source": [
                "def train():\r\n",
                "    model.train()\r\n",
                "    optimizer.zero_grad()\r\n",
                "\r\n",
                "    z = model.encode(data.edge_index, data.edge_type)\r\n",
                "\r\n",
                "    pos_out = model.decode(z, data.train_edge_index, data.train_edge_type)\r\n",
                "\r\n",
                "    neg_edge_index = negative_sampling(data.train_edge_index, data.num_nodes)\r\n",
                "    neg_out = model.decode(z, neg_edge_index, data.train_edge_type)\r\n",
                "\r\n",
                "    out = torch.cat([pos_out, neg_out])\r\n",
                "    gt = torch.cat([torch.ones_like(pos_out), torch.zeros_like(neg_out)])\r\n",
                "    cross_entropy_loss = F.binary_cross_entropy_with_logits(out, gt)\r\n",
                "    reg_loss = z.pow(2).mean() + model.decoder.rel_emb.pow(2).mean()\r\n",
                "    loss = cross_entropy_loss + 1e-2 * reg_loss\r\n",
                "\r\n",
                "    loss.backward()\r\n",
                "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\r\n",
                "    optimizer.step()\r\n",
                "\r\n",
                "    return float(loss)\r\n",
                "\r\n",
                "\r\n",
                "@torch.no_grad()\r\n",
                "def test():\r\n",
                "    model.eval()\r\n",
                "    z = model.encode(data.edge_index, data.edge_type)\r\n",
                "\r\n",
                "    valid_mrr = compute_mrr(z, data.valid_edge_index, data.valid_edge_type)\r\n",
                "    test_mrr = compute_mrr(z, data.test_edge_index, data.test_edge_type)\r\n",
                "\r\n",
                "    return valid_mrr, test_mrr\r\n",
                "\r\n",
                "\r\n",
                "@torch.no_grad()\r\n",
                "def compute_mrr(z, edge_index, edge_type):\r\n",
                "    ranks = []\r\n",
                "    for i in tqdm(range(edge_type.numel())):\r\n",
                "        (src, dst), rel = edge_index[:, i], edge_type[i]\r\n",
                "\r\n",
                "        # Try all nodes as tails, but delete true triplets:\r\n",
                "        tail_mask = torch.ones(data.num_nodes, dtype=torch.bool)\r\n",
                "        for (heads, tails), types in [\r\n",
                "            (data.train_edge_index, data.train_edge_type),\r\n",
                "            (data.valid_edge_index, data.valid_edge_type),\r\n",
                "            (data.test_edge_index, data.test_edge_type),\r\n",
                "        ]:\r\n",
                "            tail_mask[tails[(heads == src) & (types == rel)]] = False\r\n",
                "\r\n",
                "        tail = torch.arange(data.num_nodes)[tail_mask]\r\n",
                "        tail = torch.cat([torch.tensor([dst]), tail])\r\n",
                "        head = torch.full_like(tail, fill_value=src)\r\n",
                "        eval_edge_index = torch.stack([head, tail], dim=0)\r\n",
                "        eval_edge_type = torch.full_like(tail, fill_value=rel)\r\n",
                "\r\n",
                "        out = model.decode(z, eval_edge_index, eval_edge_type)\r\n",
                "        perm = out.argsort(descending=True)\r\n",
                "        rank = int((perm == 0).nonzero(as_tuple=False).view(-1)[0])\r\n",
                "        ranks.append(rank + 1)\r\n",
                "\r\n",
                "        # Try all nodes as heads, but delete true triplets:\r\n",
                "        head_mask = torch.ones(data.num_nodes, dtype=torch.bool)\r\n",
                "        for (heads, tails), types in [\r\n",
                "            (data.train_edge_index, data.train_edge_type),\r\n",
                "            (data.valid_edge_index, data.valid_edge_type),\r\n",
                "            (data.test_edge_index, data.test_edge_type),\r\n",
                "        ]:\r\n",
                "            head_mask[heads[(tails == dst) & (types == rel)]] = False\r\n",
                "\r\n",
                "        head = torch.arange(data.num_nodes)[head_mask]\r\n",
                "        head = torch.cat([torch.tensor([src]), head])\r\n",
                "        tail = torch.full_like(head, fill_value=dst)\r\n",
                "        eval_edge_index = torch.stack([head, tail], dim=0)\r\n",
                "        eval_edge_type = torch.full_like(head, fill_value=rel)\r\n",
                "\r\n",
                "        out = model.decode(z, eval_edge_index, eval_edge_type)\r\n",
                "        perm = out.argsort(descending=True)\r\n",
                "        rank = int((perm == 0).nonzero(as_tuple=False).view(-1)[0])\r\n",
                "        ranks.append(rank + 1)\r\n",
                "\r\n",
                "    return (1. / torch.tensor(ranks, dtype=torch.float)).mean()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "# 경고: 정말 오래 걸린다.\r\n",
                "for epoch in range(1, 1001):\r\n",
                "    loss = train()\r\n",
                "    print(f'Epoch: {epoch:05d}, Loss: {loss:.4f}')\r\n",
                "    if (epoch % 500) == 0:\r\n",
                "        valid_mrr, test_mrr = test()\r\n",
                "        print(f'Val MRR: {valid_mrr:.4f}, Test MRR: {test_mrr:.4f}')"
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.6 64-bit ('venv': venv)"
        },
        "interpreter": {
            "hash": "fa39ba5ccb812d92595bf6f9930702a610785e442d2dfe30e0b5b2dee928db5d"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}