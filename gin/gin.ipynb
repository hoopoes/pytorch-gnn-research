{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "## Graph Neural Networks with Pytorch\r\n",
                "## Target: Graph Isomorphism Networks\r\n",
                "- Original Paper: https://arxiv.org/abs/1810.00826\r\n",
                "- Original Code: https://github.com/rusty1s/pytorch_geometric/blob/master/examples/mutag_gin.py"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import os\r\n",
                "import sys\r\n",
                "\r\n",
                "import torch\r\n",
                "import torch.nn.functional as F\r\n",
                "from torch.nn import Sequential, Linear, BatchNorm1d, ReLU\r\n",
                "from torch_geometric.datasets import TUDataset\r\n",
                "from torch_geometric.data import DataLoader\r\n",
                "from torch_geometric.nn import GINConv, global_add_pool\r\n",
                "\r\n",
                "sys.path.append('../')\r\n",
                "from utils import *\r\n",
                "logger = make_logger(name='gin_logger')"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "# Load TU Dataset\r\n",
                "dataset = 'TU'\r\n",
                "path = os.path.join(os.getcwd(), '..', 'data', dataset)\r\n",
                "dataset = TUDataset(path, name='MUTAG').shuffle()\r\n",
                "\r\n",
                "# dataset length: 188\r\n",
                "logger.info(f\"Dataset Length: {len(dataset)}\")\r\n",
                "\r\n",
                "train_dataset = dataset[len(dataset) // 10:]\r\n",
                "test_dataset = dataset[:len(dataset) // 10]\r\n",
                "\r\n",
                "len_train, len_test = len(train_dataset), len(test_dataset)\r\n",
                "logger.info(\"Train:Test: {:.4f}:{:.4f}\".format(\r\n",
                "    len_train/(len_train+len_test), len_test/(len_train+len_test)))\r\n",
                "\r\n",
                "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\r\n",
                "test_loader = DataLoader(test_dataset, batch_size=128)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "2021-08-09 22:35:15,349 - gin_logger - Dataset Length: 188\n",
                        "2021-08-09 22:35:15,350 - gin_logger - Train:Test: 0.9043:0.0957\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "# Define Model\r\n",
                "class GIN(torch.nn.Module):\r\n",
                "    def __init__(self, in_channels, dim, out_channels):\r\n",
                "        super(GIN, self).__init__()\r\n",
                "\r\n",
                "        self.conv1 = GINConv(\r\n",
                "            nn=Sequential(\r\n",
                "                Linear(in_channels, dim),\r\n",
                "                BatchNorm1d(dim),\r\n",
                "                ReLU(),\r\n",
                "                Linear(dim, dim),\r\n",
                "                ReLU())\r\n",
                "        )\r\n",
                "\r\n",
                "        self.conv2 = GINConv(\r\n",
                "            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\r\n",
                "                       Linear(dim, dim), ReLU()))\r\n",
                "\r\n",
                "        self.conv3 = GINConv(\r\n",
                "            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\r\n",
                "                       Linear(dim, dim), ReLU()))\r\n",
                "\r\n",
                "        self.conv4 = GINConv(\r\n",
                "            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\r\n",
                "                       Linear(dim, dim), ReLU()))\r\n",
                "\r\n",
                "        self.conv5 = GINConv(\r\n",
                "            Sequential(Linear(dim, dim), BatchNorm1d(dim), ReLU(),\r\n",
                "                       Linear(dim, dim), ReLU()))\r\n",
                "\r\n",
                "        self.lin1 = Linear(dim, dim)\r\n",
                "        self.lin2 = Linear(dim, out_channels)\r\n",
                "\r\n",
                "    def forward(self, x, edge_index, batch):\r\n",
                "        x = self.conv1(x, edge_index)\r\n",
                "        x = self.conv2(x, edge_index)\r\n",
                "        x = self.conv3(x, edge_index)\r\n",
                "        x = self.conv4(x, edge_index)\r\n",
                "        x = self.conv5(x, edge_index)\r\n",
                "        x = global_add_pool(x, batch)\r\n",
                "        x = self.lin1(x).relu()\r\n",
                "        x = F.dropout(x, p=0.5, training=self.training)\r\n",
                "        x = self.lin2(x)\r\n",
                "        return F.log_softmax(x, dim=-1)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "**Graph Isomorphism Networks**는 Graph Neural Networks 연구와 실제 적용에 있어서 굉장한 의미를 지니는 논문이다.  \r\n",
                "본 방법론의 경우 기존의 알고리즘의 여러 한계점을 개선하면서도 심플한 구조를 지니고 있기에 구현 및 적용에 있어서도 상당히 용이한 편이다.  \r\n",
                "\r\n",
                "**GINConv** Layer는 아래와 같은 구조를 갖는다. [공식 문서](https://pytorch-geometric.readthedocs.io/en/latest/modules/nn.html#torch_geometric.nn.conv.GINConv)를 참고하면 좋다.  \r\n",
                "\r\n",
                "$$ h_v^k = MLP^k ( (1 + \\epsilon^k) \\cdot h_v^{k-1} + \\Sigma_{u \\in \\mathcal{N} (v)} h_u^{k-1} ) $$  \r\n",
                "\r\n",
                "여기서 $\\epsilon$ 의 경우 `train_eps`라는 인자를 `True`로 설정하면 학습 가능한 파라미터로 설정된다.  \r\n",
                "`eps` 인자로 초기값을 설정할 수 있는데, 그냥 기본 값인 0으로 두는 것이 낫다.  \r\n",
                "데이터를 비롯한 여러 조건에 따라 다르겠지만, 필자의 경험과 원 논문에서 기술한 부분을 고려했을 때 $\\epsilon$ 은 제거하는 편을 추천하는 바이다.  \r\n",
                "\r\n",
                "`nn` 인자에 Base가 되는 Neural Network를 입력해주면 된다.  \r\n",
                "\r\n",
                "위 예시에서 처럼 최소 2개의 Layer를 갖는 MLP를 두는 것이 좋으나, 데이터의 성격에 따라 Single-layer Perceptron으로도 충분한 경우도 있다.  \r\n",
                "\r\n",
                "위 예시에서는 **GINConv** Layer를 5번 사용하였지만 이 역시 학습을 진행하면서 여러 시도를 해보는 것이 좋다.  \r\n",
                "\r\n",
                "참고로 **Bipartite Graph** 구조를 가진 대용량의 데이터에 대해 **Graph Isomorphism Networks**를 `Pyspark`와 `Tensorflow`로 구현한 예시는 [이 곳](https://github.com/ocasoyy/Bipartite-Graph-Isomorphism-Network)에서 확인할 수 있다.  \r\n",
                "\r\n",
                "자세한 설명은 [GIN 논문 리뷰 글](https://greeksharifa.github.io/machine_learning/2021/06/05/GIN/)을 참조하길 바란다.  "
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "source": [
                "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\r\n",
                "model = GIN(dataset.num_features, 32, dataset.num_classes).to(device)\r\n",
                "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\r\n",
                "\r\n",
                "\r\n",
                "def train():\r\n",
                "    model.train()\r\n",
                "\r\n",
                "    total_loss = 0\r\n",
                "    for data in train_loader:\r\n",
                "        data = data.to(device)\r\n",
                "        optimizer.zero_grad()\r\n",
                "        output = model(data.x, data.edge_index, data.batch)\r\n",
                "        loss = F.nll_loss(output, data.y)\r\n",
                "        loss.backward()\r\n",
                "        optimizer.step()\r\n",
                "        total_loss += float(loss) * data.num_graphs\r\n",
                "    return total_loss / len(train_loader.dataset)\r\n",
                "\r\n",
                "\r\n",
                "@torch.no_grad()\r\n",
                "def test(loader):\r\n",
                "    model.eval()\r\n",
                "\r\n",
                "    total_correct = 0\r\n",
                "    for data in loader:\r\n",
                "        data = data.to(device)\r\n",
                "        out = model(data.x, data.edge_index, data.batch)\r\n",
                "        total_correct += int((out.argmax(-1) == data.y).sum())\r\n",
                "    return total_correct / len(loader.dataset)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "for epoch in range(1, 101):\r\n",
                "    loss = train()\r\n",
                "    train_acc = test(train_loader)\r\n",
                "    test_acc = test(test_loader)\r\n",
                "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Train Acc: {train_acc:.4f} '\r\n",
                "          f'Test Acc: {test_acc:.4f}')"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "Epoch: 001, Loss: 0.8721, Train Acc: 0.6647 Test Acc: 0.6667\n",
                        "Epoch: 002, Loss: 0.5220, Train Acc: 0.6647 Test Acc: 0.6667\n",
                        "Epoch: 003, Loss: 0.4124, Train Acc: 0.6647 Test Acc: 0.6667\n",
                        "Epoch: 004, Loss: 0.4133, Train Acc: 0.6647 Test Acc: 0.6667\n",
                        "Epoch: 005, Loss: 0.3894, Train Acc: 0.6647 Test Acc: 0.6667\n",
                        "Epoch: 006, Loss: 0.3725, Train Acc: 0.6647 Test Acc: 0.6667\n",
                        "Epoch: 007, Loss: 0.3544, Train Acc: 0.6588 Test Acc: 0.7222\n",
                        "Epoch: 008, Loss: 0.3324, Train Acc: 0.7118 Test Acc: 0.7778\n",
                        "Epoch: 009, Loss: 0.3297, Train Acc: 0.7059 Test Acc: 0.8889\n",
                        "Epoch: 010, Loss: 0.3161, Train Acc: 0.7176 Test Acc: 0.8889\n",
                        "Epoch: 011, Loss: 0.3222, Train Acc: 0.7529 Test Acc: 0.9444\n",
                        "Epoch: 012, Loss: 0.3148, Train Acc: 0.7471 Test Acc: 0.8889\n",
                        "Epoch: 013, Loss: 0.2780, Train Acc: 0.7882 Test Acc: 0.8333\n",
                        "Epoch: 014, Loss: 0.2772, Train Acc: 0.8176 Test Acc: 0.8889\n",
                        "Epoch: 015, Loss: 0.2953, Train Acc: 0.8176 Test Acc: 0.9444\n",
                        "Epoch: 016, Loss: 0.2745, Train Acc: 0.8000 Test Acc: 1.0000\n",
                        "Epoch: 017, Loss: 0.3030, Train Acc: 0.3353 Test Acc: 0.3333\n",
                        "Epoch: 018, Loss: 0.2834, Train Acc: 0.3588 Test Acc: 0.3333\n",
                        "Epoch: 019, Loss: 0.3082, Train Acc: 0.8059 Test Acc: 0.9444\n",
                        "Epoch: 020, Loss: 0.3245, Train Acc: 0.8412 Test Acc: 0.9444\n",
                        "Epoch: 021, Loss: 0.3200, Train Acc: 0.7824 Test Acc: 0.8889\n",
                        "Epoch: 022, Loss: 0.2936, Train Acc: 0.7706 Test Acc: 0.8889\n",
                        "Epoch: 023, Loss: 0.2893, Train Acc: 0.7647 Test Acc: 0.8889\n",
                        "Epoch: 024, Loss: 0.3137, Train Acc: 0.8118 Test Acc: 0.9444\n",
                        "Epoch: 025, Loss: 0.2671, Train Acc: 0.8529 Test Acc: 0.9444\n",
                        "Epoch: 026, Loss: 0.2546, Train Acc: 0.8471 Test Acc: 0.9444\n",
                        "Epoch: 027, Loss: 0.2496, Train Acc: 0.8176 Test Acc: 0.8889\n",
                        "Epoch: 028, Loss: 0.2432, Train Acc: 0.7824 Test Acc: 0.8889\n",
                        "Epoch: 029, Loss: 0.2638, Train Acc: 0.7706 Test Acc: 0.8889\n",
                        "Epoch: 030, Loss: 0.2205, Train Acc: 0.7471 Test Acc: 0.8333\n",
                        "Epoch: 031, Loss: 0.2533, Train Acc: 0.7235 Test Acc: 0.8889\n",
                        "Epoch: 032, Loss: 0.2375, Train Acc: 0.7294 Test Acc: 0.8889\n",
                        "Epoch: 033, Loss: 0.2522, Train Acc: 0.7882 Test Acc: 0.8889\n",
                        "Epoch: 034, Loss: 0.2532, Train Acc: 0.8000 Test Acc: 0.8889\n",
                        "Epoch: 035, Loss: 0.2729, Train Acc: 0.8353 Test Acc: 0.9444\n",
                        "Epoch: 036, Loss: 0.2352, Train Acc: 0.8824 Test Acc: 0.9444\n",
                        "Epoch: 037, Loss: 0.2652, Train Acc: 0.8824 Test Acc: 0.9444\n",
                        "Epoch: 038, Loss: 0.2185, Train Acc: 0.9000 Test Acc: 0.9444\n",
                        "Epoch: 039, Loss: 0.2285, Train Acc: 0.9176 Test Acc: 1.0000\n",
                        "Epoch: 040, Loss: 0.2367, Train Acc: 0.9235 Test Acc: 1.0000\n",
                        "Epoch: 041, Loss: 0.2061, Train Acc: 0.8882 Test Acc: 0.9444\n",
                        "Epoch: 042, Loss: 0.1871, Train Acc: 0.8412 Test Acc: 0.9444\n",
                        "Epoch: 043, Loss: 0.2129, Train Acc: 0.8471 Test Acc: 1.0000\n",
                        "Epoch: 044, Loss: 0.2039, Train Acc: 0.8647 Test Acc: 1.0000\n",
                        "Epoch: 045, Loss: 0.2178, Train Acc: 0.8294 Test Acc: 0.9444\n",
                        "Epoch: 046, Loss: 0.2022, Train Acc: 0.8000 Test Acc: 0.8889\n",
                        "Epoch: 047, Loss: 0.1954, Train Acc: 0.7941 Test Acc: 0.9444\n",
                        "Epoch: 048, Loss: 0.1879, Train Acc: 0.8294 Test Acc: 0.9444\n",
                        "Epoch: 049, Loss: 0.2008, Train Acc: 0.8176 Test Acc: 0.9444\n",
                        "Epoch: 050, Loss: 0.1863, Train Acc: 0.7941 Test Acc: 0.9444\n",
                        "Epoch: 051, Loss: 0.2019, Train Acc: 0.7941 Test Acc: 0.9444\n",
                        "Epoch: 052, Loss: 0.1816, Train Acc: 0.7882 Test Acc: 0.9444\n",
                        "Epoch: 053, Loss: 0.1915, Train Acc: 0.8471 Test Acc: 0.9444\n",
                        "Epoch: 054, Loss: 0.1905, Train Acc: 0.8588 Test Acc: 0.9444\n",
                        "Epoch: 055, Loss: 0.1974, Train Acc: 0.8882 Test Acc: 0.8889\n",
                        "Epoch: 056, Loss: 0.1731, Train Acc: 0.8882 Test Acc: 0.9444\n",
                        "Epoch: 057, Loss: 0.1972, Train Acc: 0.8882 Test Acc: 0.9444\n",
                        "Epoch: 058, Loss: 0.2000, Train Acc: 0.9000 Test Acc: 0.9444\n",
                        "Epoch: 059, Loss: 0.1721, Train Acc: 0.9118 Test Acc: 0.9444\n",
                        "Epoch: 060, Loss: 0.2269, Train Acc: 0.8882 Test Acc: 0.8889\n",
                        "Epoch: 061, Loss: 0.1771, Train Acc: 0.8882 Test Acc: 0.8889\n",
                        "Epoch: 062, Loss: 0.1881, Train Acc: 0.8588 Test Acc: 0.8333\n",
                        "Epoch: 063, Loss: 0.2226, Train Acc: 0.7588 Test Acc: 0.6667\n",
                        "Epoch: 064, Loss: 0.1832, Train Acc: 0.6765 Test Acc: 0.6111\n",
                        "Epoch: 065, Loss: 0.1967, Train Acc: 0.6294 Test Acc: 0.5000\n",
                        "Epoch: 066, Loss: 0.1977, Train Acc: 0.6000 Test Acc: 0.5000\n",
                        "Epoch: 067, Loss: 0.1891, Train Acc: 0.6412 Test Acc: 0.4444\n",
                        "Epoch: 068, Loss: 0.1998, Train Acc: 0.6824 Test Acc: 0.5556\n",
                        "Epoch: 069, Loss: 0.1694, Train Acc: 0.7176 Test Acc: 0.5000\n",
                        "Epoch: 070, Loss: 0.1733, Train Acc: 0.7941 Test Acc: 0.7222\n",
                        "Epoch: 071, Loss: 0.1631, Train Acc: 0.8706 Test Acc: 0.8333\n",
                        "Epoch: 072, Loss: 0.1657, Train Acc: 0.9471 Test Acc: 0.9444\n",
                        "Epoch: 073, Loss: 0.2073, Train Acc: 0.9176 Test Acc: 0.9444\n",
                        "Epoch: 074, Loss: 0.1942, Train Acc: 0.8882 Test Acc: 0.7778\n",
                        "Epoch: 075, Loss: 0.2090, Train Acc: 0.8647 Test Acc: 0.8333\n",
                        "Epoch: 076, Loss: 0.2315, Train Acc: 0.8353 Test Acc: 0.9444\n",
                        "Epoch: 077, Loss: 0.2169, Train Acc: 0.7647 Test Acc: 0.8889\n",
                        "Epoch: 078, Loss: 0.3151, Train Acc: 0.7824 Test Acc: 0.8333\n",
                        "Epoch: 079, Loss: 0.2582, Train Acc: 0.7824 Test Acc: 0.7778\n",
                        "Epoch: 080, Loss: 0.2527, Train Acc: 0.8000 Test Acc: 0.7222\n",
                        "Epoch: 081, Loss: 0.2636, Train Acc: 0.8235 Test Acc: 0.8333\n",
                        "Epoch: 082, Loss: 0.2630, Train Acc: 0.8294 Test Acc: 0.8333\n",
                        "Epoch: 083, Loss: 0.2518, Train Acc: 0.8647 Test Acc: 0.9444\n",
                        "Epoch: 084, Loss: 0.2460, Train Acc: 0.8706 Test Acc: 0.9444\n",
                        "Epoch: 085, Loss: 0.2478, Train Acc: 0.8824 Test Acc: 0.9444\n",
                        "Epoch: 086, Loss: 0.2858, Train Acc: 0.8824 Test Acc: 0.9444\n",
                        "Epoch: 087, Loss: 0.2199, Train Acc: 0.9000 Test Acc: 0.9444\n",
                        "Epoch: 088, Loss: 0.2132, Train Acc: 0.8882 Test Acc: 0.9444\n",
                        "Epoch: 089, Loss: 0.2282, Train Acc: 0.8882 Test Acc: 1.0000\n",
                        "Epoch: 090, Loss: 0.2567, Train Acc: 0.8941 Test Acc: 1.0000\n",
                        "Epoch: 091, Loss: 0.2086, Train Acc: 0.8529 Test Acc: 1.0000\n",
                        "Epoch: 092, Loss: 0.2508, Train Acc: 0.8647 Test Acc: 0.9444\n",
                        "Epoch: 093, Loss: 0.2165, Train Acc: 0.8824 Test Acc: 0.9444\n",
                        "Epoch: 094, Loss: 0.2134, Train Acc: 0.9059 Test Acc: 0.9444\n",
                        "Epoch: 095, Loss: 0.2079, Train Acc: 0.8882 Test Acc: 0.9444\n",
                        "Epoch: 096, Loss: 0.2117, Train Acc: 0.8706 Test Acc: 0.9444\n",
                        "Epoch: 097, Loss: 0.1955, Train Acc: 0.8353 Test Acc: 0.9444\n",
                        "Epoch: 098, Loss: 0.2120, Train Acc: 0.8412 Test Acc: 0.9444\n",
                        "Epoch: 099, Loss: 0.1763, Train Acc: 0.8588 Test Acc: 1.0000\n",
                        "Epoch: 100, Loss: 0.1868, Train Acc: 0.8882 Test Acc: 0.9444\n"
                    ]
                }
            ],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.6 64-bit ('venv': venv)"
        },
        "interpreter": {
            "hash": "a0c8d12882c844609327ff267203d9c0214a9c81f212141ad6ff46d6cf6ed682"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}