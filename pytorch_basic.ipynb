{
    "cells": [
        {
            "cell_type": "markdown",
            "source": [
                "## Pytorch Common Knowledge"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "import torch\r\n",
                "from torch import nn, Tensor\r\n",
                "import torch.nn.functional as F"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 1. view, reshape, transpose, permute 차이  "
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "[contiguous 의미 설명 글](https://stackoverflow.com/questions/26998223/what-is-the-difference-between-contiguous-and-non-contiguous-arrays/26999092#26999092)  \r\n",
                "\r\n",
                "C contiguous는 rows가 contiguous blocks of memory로 저장되었음을 의미한다.  \r\n",
                "즉, 다음 memory address가 그 row의 다음 row value를 갖고 있다는 것이다.  \r\n",
                "(column 방향은 Fortran contiguous)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "a = torch.FloatTensor([[1,2,3], [4,5,6]])\r\n",
                "b = torch.transpose(a, 1, 0)\r\n",
                "# memory space 확인: a.storage().data_ptr()"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "# C contiguity is lost\r\n",
                "print(a.is_contiguous(), b.is_contiguous())"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "True False\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "# view는 contiguity를 보존하지만 reshape은 그렇지 않음\r\n",
                "a_view = a.view(3,2)\r\n",
                "a_reshape = a.reshape(3,2)\r\n",
                "print(a.is_contiguous())\r\n",
                "print(b.is_contiguous())"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "True\n",
                        "False\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "transpose는 0, 1 사이만 바꿀 수 있지만, permute는 훨씬 자유롭게 고차원까지 커버할 수 있음  \r\n",
                "\r\n",
                "memory efficiency를 유지하기 위해서는 reshape, transpose, permute를 사용한 후에 아래와 같이 해주는 것이 좋음"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "a_transpose = torch.transpose(a, 1, 0)\r\n",
                "print(a_transpose.is_contiguous())\r\n",
                "a_transpose = torch.transpose(a, 1, 0).contiguous()\r\n",
                "print(a_transpose.is_contiguous())"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "False\n",
                        "True\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 2. register_buffer"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "import math\r\n",
                "\r\n",
                "class PositionalEncoding(nn.Module):\r\n",
                "    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):\r\n",
                "        super().__init__()\r\n",
                "        self.dropout = nn.Dropout(p=dropout)\r\n",
                "\r\n",
                "        position = torch.arange(max_len).unsqueeze(1)\r\n",
                "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\r\n",
                "        pe = torch.zeros(max_len, 1, d_model)\r\n",
                "        pe[:, 0, 0::2] = torch.sin(position * div_term)\r\n",
                "        pe[:, 0, 1::2] = torch.cos(position * div_term)\r\n",
                "        self.register_buffer(name='pe', tensor=pe, persistent=True)\r\n",
                "\r\n",
                "    def forward(self, x: Tensor) -> Tensor:\r\n",
                "        # x: Tensor, shape [seq_len, batch_size, embedding_dim]\r\n",
                "        x = x + self.pe[:x.size(0)]\r\n",
                "        return self.dropout(x)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "torch.nn.Module에 register_buffer를 적용하면, 특정 Tensor나 Layer를 파라미터로 취급하지 않게 해준다.  \r\n",
                "예를 들어 BatchNorm의 `running_mean` 은 학습 가능한 파라미터가 아니지만 Module's state의 일부이다.  \r\n",
                "\r\n",
                "`persistent` 인자를 `False`로 하면 추후에 module의 `state_dict()` 메서드에서 등장하지 않는다.  \r\n",
                "\r\n",
                "또한 optimizer로 업데이트하지 않고, GPU 연산이 가능하다.  "
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "# self.register_buffer('running_mean', torch.zeros(num_features))\r\n",
                "pe = PositionalEncoding(d_model=512)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "source": [
                "print(pe.state_dict().keys())"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "odict_keys(['pe'])\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "source": [
                "print([p for p in pe.parameters()])"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 3. einsum  \r\n",
                "[공식 문서 링크](https://pytorch.org/docs/stable/generated/torch.einsum.html)  \r\n",
                "[참고 블로그](https://ita9naiwa.github.io/numeric%20calculation/2018/11/10/Einsum.html)"
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "Pytorch, Tensorflow, Numpy 등에 있는 다양한 함수/메서드를 다 외우거나 일일히 찾아보지 않아도 여러 계산들을 편리하게 해주는 도구이다. `einsum` 연산은 Einstein Summation Convention에 따라 연산을 진행하는 방식이라고 한다.  "
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "import numpy as np\r\n",
                "\r\n",
                "A = np.array([[1,2,3], [4,5,6]])\r\n",
                "print(A)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[[1 2 3]\n",
                        " [4 5 6]]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "# Transpose\r\n",
                "R = np.einsum(\"ij->ji\", A)\r\n",
                "print(R)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[[1 4]\n",
                        " [2 5]\n",
                        " [3 6]]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "source": [
                "# Diagonal\r\n",
                "A = np.ones((5, 5))\r\n",
                "diag = np.einsum(\"ii->i\", A)\r\n",
                "print(diag)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[1. 1. 1. 1. 1.]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "# Trace\r\n",
                "print(np.einsum(\"ii->\", A))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "5.0\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "# Summation\r\n",
                "A = np.array([[1,2,3], [4,5,6]])\r\n",
                "R = np.einsum(\"ij->\", A)\r\n",
                "print(A)\r\n",
                "print(R)\r\n",
                "\r\n",
                "row_sum = np.einsum(\"ij->i\", A)\r\n",
                "col_sum = np.einsum(\"ij->j\", A)\r\n",
                "print(row_sum, col_sum)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[[1 2 3]\n",
                        " [4 5 6]]\n",
                        "21\n",
                        "[ 6 15] [5 7 9]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 19,
            "source": [
                "A = np.array(list(range(0, 12))).reshape((3, 2, 2))\r\n",
                "print(A)\r\n",
                "\r\n",
                "# jk -> k가 됨\r\n",
                "print(np.einsum(\"ijk->ik\", A))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "[[[ 0  1]\n",
                        "  [ 2  3]]\n",
                        "\n",
                        " [[ 4  5]\n",
                        "  [ 6  7]]\n",
                        "\n",
                        " [[ 8  9]\n",
                        "  [10 11]]]\n",
                        "[[ 2  4]\n",
                        " [10 12]\n",
                        " [18 20]]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 27,
            "source": [
                "# multiplication\r\n",
                "import torch\r\n",
                "\r\n",
                "# outer product\r\n",
                "x = torch.Tensor([1, 2, 3, 4])\r\n",
                "y = torch.Tensor([1, 1, 1, 1])\r\n",
                "print(torch.einsum(\"i,j->ij\", x, y))\r\n",
                "\r\n",
                "# inner product\r\n",
                "print(torch.einsum(\"i,j->\", x, y))\r\n",
                "\r\n",
                "# element wise multiplication\r\n",
                "print(torch.einsum(\"i,i->i\", x, y))\r\n",
                "\r\n",
                "# matrix-vector multiplication\r\n",
                "c = torch.Tensor([[2,2,2,2], [3,3,3,3]])\r\n",
                "print(torch.einsum(\"ij,j->i\", c, y))"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "tensor([[1., 1., 1., 1.],\n",
                        "        [2., 2., 2., 2.],\n",
                        "        [3., 3., 3., 3.],\n",
                        "        [4., 4., 4., 4.]])\n",
                        "tensor(40.)\n",
                        "tensor([1., 2., 3., 4.])\n",
                        "tensor([ 8., 12.])\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 29,
            "source": [
                "# Batch matrix multiplication\r\n",
                "# b=3, i=2, j=5, k=4\r\n",
                "# b -> batch size, j -> 공통 dimension\r\n",
                "# 결과는 (3, 2, 4)가 되어야겠지?\r\n",
                "As = torch.randn(3,2,5)\r\n",
                "Bs = torch.randn(3,5,4)\r\n",
                "\r\n",
                "result = torch.einsum(\"bij,bjk->bik\", As, Bs)\r\n",
                "print(result.shape)"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "torch.Size([3, 2, 4])\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "markdown",
            "source": [
                "## 4. next"
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.6",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.6 64-bit ('venv': venv)"
        },
        "interpreter": {
            "hash": "fa39ba5ccb812d92595bf6f9930702a610785e442d2dfe30e0b5b2dee928db5d"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}